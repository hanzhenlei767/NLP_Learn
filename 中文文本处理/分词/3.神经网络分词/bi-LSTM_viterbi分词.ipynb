{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bi-LSTM+viterbi分词\n",
    "* 使用TensorFlow大家bi-LSTM神经网络，实现文本序列输入-》标记序列输出\n",
    "* 使用viterbi对标记序列规范化\n",
    "* 初始状态：人为设定'b','s'为有效，'e'，'m'无效\n",
    "* 状态转移矩阵：人为设定，符合规则的设置成0.5，不符合规则的设置成0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义训练样本处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    \"\"\"\n",
    "    读取语料文件，将字和标记分开以list形式存储，形式[[字],[标记]]\n",
    "    :param data_path:语料文件路径，文件格式：字1/标记1  字2/标记2  \n",
    "    :return content:  [[字1,标记1],[字2,标记2],...]\n",
    "    \"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        word_list = f.read().split()\n",
    "    all_list = [[word.split('/')[0], word.split('/')[1]] for word in word_list]\n",
    "    return all_list\n",
    "\n",
    "# 制作word2id,id2word,tag2id\n",
    "def make_dict(all_list):\n",
    "    \"\"\"\n",
    "    统计语料中字和标记，生成字和标记的编码\n",
    "    :param all_list:输入数据，格式：[[字1,标记1],[字2,标记2],...]\n",
    "    :return word2id, id2word, tag2id: 字：列表值，列表值：字，标记：列表值\n",
    "    \"\"\"\n",
    "    ##统计字和标记\n",
    "    all_char = [] #以hash的形式存储字符\n",
    "    all_tag = [] #以hash的形式存储标记\n",
    "    for i in all_list:\n",
    "        if i[0] not in all_char:\n",
    "            all_char.append(i[0])\n",
    "        if i[1] not in all_tag:\n",
    "            all_tag.append(i[1])\n",
    "\n",
    "    #添加不认识的字，或者pad的字\n",
    "    all_char.append('<UNK>')\n",
    "    all_char.append('<PAD>')\n",
    "    #添加非字对应的标记\n",
    "    all_tag.append('x')\n",
    "    print(all_tag)\n",
    "    \n",
    "    ##为字和标记编码成列表\n",
    "    word2id = {}#字：列表值\n",
    "    id2word = {}#列表值：字\n",
    "    tag2id = {}#标记：列表值\n",
    " \n",
    "    for index, char in enumerate(all_char):\n",
    "        word2id[char] = index\n",
    "        id2word[index] = char\n",
    "    for index, char in enumerate(all_tag):\n",
    "        tag2id[char] = index\n",
    "    return word2id, id2word, tag2id\n",
    "\n",
    "\n",
    "def data_util(data_path,word2id, tag2id):\n",
    "    \"\"\"\n",
    "    将样本转换成字和标记的列表值\n",
    "    :param data_path:语料文件路径\n",
    "    :param word2id:字：列表值\n",
    "    :param tag2id:标记：列表值\n",
    "    :return all_list: 一条样本格式：[[字1,标记1],[字2,标记2],...,[字n,标记n],句子长度]\n",
    "    \"\"\"\n",
    "    with open(data_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "    rr = re.compile(r'[,，。、“”‘’－》《（）●：！;…？]/s')\n",
    "    sentences = rr.split(data)#以上式作为句子的分隔符，分隔出字符句子\n",
    "    sentences = list(filter(lambda x: x.strip(), sentences))#过滤掉空句子\n",
    "    sentences = list(map(lambda x: x.strip(), sentences))#将字符串句子的前后空格去掉\n",
    "\n",
    "    all_list = []\n",
    "    for i in sentences:\n",
    "        word_list = i.split()#默认以空格分隔\n",
    "        one_list = [[word2id[word.split('/')[0]], tag2id[word.split('/')[1]]] for word in word_list]\n",
    "        one_list.append(len(word_list))#计算记录样本的长度\n",
    "        all_list.append(one_list)\n",
    "    return all_list\n",
    "\n",
    "# 产生随机的embedding矩阵\n",
    "def random_embedding(word2id, embedding_dim):\n",
    "    \"\"\"\n",
    "    生成嵌入层的矩阵，初始化成随机值。\n",
    "    :param id2word:列表：字-》统计所有字个数\n",
    "    :param embedding_dim:词向量维度\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embedding_mat = np.random.uniform(-0.25, 0.25, (len(word2id), embedding_dim))\n",
    "    embedding_mat = np.float32(embedding_mat)\n",
    "    return embedding_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'zu_data\\data.txt'#训练样本路径\n",
    "all_list = read_data(data_path)#将样本处理成[字，标]类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'e', 's', 'm', 'x']\n"
     ]
    }
   ],
   "source": [
    "word2id, id2word, tag2id = make_dict(all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_util(data_path, word2id, tag2id)\n",
    "\n",
    "#拆分数据为训练集和验证集\n",
    "train_set = data[:-1000]\n",
    "test_set = data[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建bi-LSTM神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有参数\n",
    "hidden_size = 128\n",
    "batch_size = 512\n",
    "cell_nums = 2#LSTM的层数\n",
    "epoch_num = 1\n",
    "optimizer = 'Adam'#定义优化器\n",
    "lr = 0.001\n",
    "clip = 5.0\n",
    "dropout = 1\n",
    "num_tags = 5#标签维度\n",
    "update_embedding = True\n",
    "embedding_dim = 200\n",
    "shuffle = False\n",
    "isTrain = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义batch获取方法\n",
    "* 定义标签处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, tag_nums):\n",
    "    \"\"\"\n",
    "    将2维的tag转化为one-hot形式，return结果为3维\n",
    "    :param labels:标签格式：[[1,2,3],[句子的标记序列2],..]\n",
    "    :param tag_nums:标签种类个数\n",
    "    :return:标签格式：[[[0,1,0,0,0],[0,0,1,0,0],[0,0,0,1,0]],[[],[],...,[]],..]\n",
    "    \"\"\"\n",
    "    length = len(labels)#batch的样本个数\n",
    "    len_lab = len(labels[0])#第一条样本中字的个数\n",
    "    res = np.zeros((length, len_lab, tag_nums), dtype=np.float32)\n",
    "    for i in range(length):\n",
    "        for j in range(len_lab):\n",
    "            res[i][j][labels[i][j]] = 1.\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def get_batch(data, batch_size, word2id, tag2id, shuffle=False):\n",
    "    \"\"\"\n",
    "    将数据pad，生成batch数据返回，这里没有取余数。pad长度是batch中最大句子的长度。\n",
    "    :param data:格式：[[字1,标记1],[字2,标记2],...,[字n,标记n],句子长度]\n",
    "    :param batch_size:\n",
    "    :param vocab:\n",
    "    :param shuffle:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 乱序没有加\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    pad = word2id['<PAD>']\n",
    "    tag_pad = tag2id[\"x\"]\n",
    "    \n",
    "    for i in range(len(data) // batch_size):\n",
    "        data_size = data[i * batch_size: (i + 1) * batch_size]\n",
    "        seqs, labels, sentence_legth = [], [], []\n",
    "        for i in data_size:\n",
    "            one_line = np.array(i[:-1])  #去除数据\n",
    "            seqs.append(one_line[:,0])   #获取字序列\n",
    "            labels.append(one_line[:,1]) #获取标签序列\n",
    "            sentence_legth.append(i[-1]) #获取句子长度\n",
    "        max_l = max(sentence_legth)      #句子长度最大的值作为pad的维度\n",
    "\n",
    "        res_seq = []#pad后的字序列\n",
    "        for sent in seqs:\n",
    "            sent_new = np.concatenate((sent, np.tile(pad, max_l - len(sent))), axis=0)  #以pad的形式补充成等长的帧数\n",
    "            res_seq.append(sent_new)\n",
    "\n",
    "        res_labels = []#pad后的标记序列\n",
    "        for label in labels:\n",
    "            label_new = np.concatenate((label, np.tile(tag_pad, max_l - len(label))), axis=0)  #以pad的形式补充成等长的帧数\n",
    "            res_labels.append(label_new)\n",
    "\n",
    "        res_labels = to_one_hot(res_labels, 5)#将标记序列one-hot处理生成最终训练标签\n",
    "        yield np.array(res_seq), res_labels, sentence_legth\n",
    "\n",
    "def get_batch1(data, batch_size, word2id, tag2id, shuffle=False):\n",
    "    \"\"\"\n",
    "    将数据pad，生成batch数据返回，这里没有取余数。pad长度是batch中指定的句子长度。\n",
    "    :param data:格式：[[字1,标记1],[字2,标记2],...,[字n,标记n],句子长度]\n",
    "    :param batch_size:\n",
    "    :param vocab:\n",
    "    :param shuffle:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 乱序没有加\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    pad = word2id['<PAD>']\n",
    "    tag_pad = tag2id[\"x\"]\n",
    "    for i in range(len(data) // batch_size):\n",
    "        data_size = data[i * batch_size: (i + 1) * batch_size]\n",
    "        seqs, labels, sentence_legth = [], [], []\n",
    "        for i in data_size:\n",
    "            one_line = np.array(i[:-1])\n",
    "            seqs.append(one_line[:, 0])\n",
    "            labels.append(one_line[:, 1])\n",
    "            sentence_legth.append(i[-1])\n",
    "        max_l = max(sentence_legth)\n",
    "        res_seq = []\n",
    "        for sent in seqs:\n",
    "            if len(sent)>=32:\n",
    "                sent_new = sent[:32]\n",
    "            else:\n",
    "                sent_new = np.concatenate((sent, np.tile(pad, 32 - len(sent))), axis=0)  # 以pad的形式补充成等长的帧数\n",
    "            res_seq.append(sent_new)\n",
    "        res_labels = []\n",
    "        for label in labels:\n",
    "            if len(label)>=32:\n",
    "                label_new = label[:32]\n",
    "            else:\n",
    "                label_new = np.concatenate((label, np.tile(tag_pad, 32 - len(label))), axis=0)  # 以pad的形式补充成等长的帧数\n",
    "            res_labels.append(label_new)\n",
    "        res_labels = to_one_hot(res_labels, 5)\n",
    "        yield np.array(res_seq), res_labels, sentence_legth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = random_embedding(word2id, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 构建神经网络结构图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"evaluation/Softmax:0\", shape=(?, ?, 5), dtype=float32)\n",
      "Tensor(\"evaluation/Cast_1:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"\n",
    "    构建神经网络的结构、损失、优化方法和评估方法\n",
    "    \"\"\"\n",
    "        \n",
    "    #模型搭建\n",
    "    \n",
    "    # shape[batch_size, sentences]\n",
    "    word_ids = tf.placeholder(tf.int32, shape=[None, None], name=\"word_ids\")\n",
    "    # shape[batch_size, sentences, labels]\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, None, num_tags], name=\"labels\")\n",
    "    # 真实序列长度：shape[batch_size,1]\n",
    "    sequence_lengths = tf.placeholder(tf.int32, shape=[None,], name=\"sequence_lengths\")\n",
    "    #dropout keep_prob\n",
    "    dropout_pl = tf.placeholder(dtype=tf.float32, shape=(), name=\"dropout\")\n",
    "\n",
    "    with tf.variable_scope(\"words\"):#命名空间\n",
    "        _word_embeddings = tf.Variable(embeddings,#shape[len_words,200]\n",
    "                                       dtype=tf.float32,\n",
    "                                       trainable=update_embedding,#嵌入层是否可以训练\n",
    "                                       name=\"_word_embeddings\")\n",
    "        word_embeddings = tf.nn.embedding_lookup(params=_word_embeddings,ids=word_ids,name=\"word_embeddings\")\n",
    "        word_embeddings = tf.nn.dropout(word_embeddings, dropout_pl)\n",
    "\n",
    "    with tf.variable_scope(\"fb-lstm\"):\n",
    "        cell_fw = [tf.nn.rnn_cell.LSTMCell(hidden_size) for _ in range(cell_nums)]\n",
    "        cell_bw = [tf.nn.rnn_cell.LSTMCell(hidden_size) for _ in range(cell_nums)]\n",
    "        rnn_cell_fw = tf.nn.rnn_cell.MultiRNNCell(cell_fw)\n",
    "        rnn_cell_bw = tf.nn.rnn_cell.MultiRNNCell(cell_bw)\n",
    "        (output_fw_seq, output_bw_seq), states = tf.nn.bidirectional_dynamic_rnn(rnn_cell_fw, rnn_cell_bw, word_embeddings,\n",
    "                                                          sequence_length=sequence_lengths, dtype=tf.float32)\n",
    "        # output的shape是[batch_size, sentences, hidden_size*2]\n",
    "        output = tf.concat([output_fw_seq, output_bw_seq], axis=-1)\n",
    "        output = tf.nn.dropout(output, dropout_pl)\n",
    "\n",
    "    with tf.variable_scope(\"classification\"):\n",
    "        # logits:shape[batch_size, sentences, num_tags]\n",
    "        logits = tf.layers.dense(output, num_tags)\n",
    "    #计算损失\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
    "        # mask的功能是产生True、False矩阵，根据最长的序列产生。类似[Ture,Ture,Ture,Ture,Ture,Ture,Ture,Ture,Ture,False]\n",
    "        mask = tf.sequence_mask(sequence_lengths)\n",
    "        # boolean_mask的作用将loss里面超过真实长度的loss去掉\n",
    "        # 如果你这样做了，写评价函数时，也需要将pad的部分去掉。\n",
    "        losses = tf.boolean_mask(losses, mask)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "\n",
    "    #选择优化器\n",
    "        \n",
    "    with tf.variable_scope(\"train_step\"):\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        global_add = global_step.assign_add(1)#用于计数\n",
    "        \n",
    "        if optimizer == 'Adam':\n",
    "            optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        elif optimizer == 'Adadelta':\n",
    "            optim = tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "        elif optimizer == 'Adagrad':\n",
    "            optim = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "        elif optimizer == 'RMSProp':\n",
    "            optim = tf.train.RMSPropOptimizer(learning_rate=lr)\n",
    "        elif optimizer == 'Momentum':\n",
    "            optim = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9)\n",
    "        elif optimizer == 'SGD':\n",
    "            optim = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        else:\n",
    "            optim = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "\n",
    "        grads_and_vars = optim.compute_gradients(loss)\n",
    "        \n",
    "        # 对梯度gradients进行裁剪，保证在[-clip, clip]之间。\n",
    "        grads_and_vars_clip = [[tf.clip_by_value(g, -clip, clip), v] for g, v in grads_and_vars]\n",
    "        \n",
    "        train_op = optim.apply_gradients(grads_and_vars_clip, global_step=global_step)\n",
    "\n",
    "    #准确率计算\n",
    "        \n",
    "    with tf.variable_scope(\"evaluation\"):\n",
    "        true_ = tf.cast(tf.argmax(labels, axis=-1), tf.float32)#真实序列的值\n",
    "        labels_softmax = tf.nn.softmax(logits)\n",
    "        print(labels_softmax)\n",
    "        labels_softmax_ = tf.argmax(logits, axis=-1)\n",
    "        pred_ = tf.cast(labels_softmax_, tf.float32)#预测序列的值\n",
    "        print(pred_)\n",
    "        zeros_like_actuals = tf.zeros_like(true_)#生成相同shape的全0的tensor\n",
    "        four_like_actuals = tf.ones_like(true_) * 4#生成相同shape的全4的tensor\n",
    "        \n",
    "        mask1 = tf.equal(tf.cast(tf.equal(four_like_actuals, true_), tf.float32), zeros_like_actuals)\n",
    "        \n",
    "        true = tf.boolean_mask(true_, mask1)\n",
    "        pred = tf.boolean_mask(pred_, mask1)\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, true), tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 运行图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, global_step 81, loss: 1.23, accuracy: 0.4292 \n",
      "epoch 1, global_step 161, loss: 1.021, accuracy: 0.5607 \n",
      "epoch 1, global_step 241, loss: 0.7721, accuracy: 0.7121 \n",
      "epoch 1, global_step 321, loss: 0.6177, accuracy: 0.7803 \n",
      "epoch 1, global_step 401, loss: 0.5184, accuracy: 0.8202 \n",
      "WARNING:tensorflow:From C:\\Users\\86153\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "epoch 1, global_step 481, loss: 0.4949, accuracy: 0.8297 \n",
      "epoch 1, global_step 561, loss: 0.5764, accuracy: 0.783 \n",
      "epoch 1, global_step 641, loss: 0.4563, accuracy: 0.8413 \n",
      "epoch 1, global_step 721, loss: 0.4548, accuracy: 0.8375 \n",
      "epoch 1, global_step 801, loss: 0.4366, accuracy: 0.8399 \n",
      "epoch 1, global_step 881, loss: 0.4149, accuracy: 0.85 \n",
      "epoch 1, global_step 961, loss: 0.3645, accuracy: 0.8726 \n",
      "epoch 1, global_step 1041, loss: 0.3687, accuracy: 0.8712 \n",
      "epoch 1, global_step 1121, loss: 0.2997, accuracy: 0.9018 \n",
      "epoch 1, global_step 1201, loss: 0.3866, accuracy: 0.8654 \n",
      "epoch 1, global_step 1281, loss: 0.363, accuracy: 0.8657 \n",
      "global_step 2, loss: 0.373, accuracy: 0.8631 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    if isTrain:\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        try:\n",
    "            ckpt_path = tf.train.latest_checkpoint('checkpoint/')\n",
    "            saver.restore(sess, ckpt_path)\n",
    "        except ValueError:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "        for epoch in range(epoch_num):\n",
    "            for res_seq, res_labels, sentence_legth in get_batch(train_set, batch_size, word2id, tag2id, shuffle=shuffle):\n",
    "                _, l, acc, global_nums, logits_, labels_ = sess.run([train_op, loss, accuracy, global_add,pred_, true_], {\n",
    "                    word_ids: res_seq,\n",
    "                    labels: res_labels,\n",
    "                    sequence_lengths: sentence_legth,\n",
    "                    dropout_pl:dropout\n",
    "                })\n",
    "                if global_nums % 80 == 0:#每20个batch保存一次\n",
    "                    saver.save(sess, './checkpoint/model.ckpt', global_step=global_nums)\n",
    "                    print('epoch {}, global_step {}, loss: {:.4}, accuracy: {:.4} '.format(epoch + 1, global_nums + 1, l, acc))\n",
    "        #验证集上测试效果\n",
    "        nums = 0\n",
    "        for res_seq, res_labels, sentence_legth in get_batch(test_set, batch_size,word2id, tag2id,shuffle=shuffle):\n",
    "            l, acc = sess.run([loss, accuracy], {\n",
    "                    word_ids: res_seq,\n",
    "                    labels: res_labels,\n",
    "                    sequence_lengths: sentence_legth,\n",
    "                    dropout_pl: dropout\n",
    "                })\n",
    "            nums += 1\n",
    "            if nums % 1 == 0:\n",
    "                print('global_step {}, loss: {:.4}, accuracy: {:.4} '.format(nums + 1, l, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 加载一个预训练的模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/model.ckpt-1200\n",
      "谢娜与子怡正面较量，国外谁的名气更大，看到老外的反应就知道了\n",
      "谢娜  与  谢娜与子怡  正面  较量  ，国  ，国外  谁  的  名气  更  大，  看到  老外  的  反应  就  知道  了  \n",
      "['b', 'e', 's', 'm', 'e', 'b', 'e', 'b', 'e', 'b', 'e', 'e', 's', 's', 'b', 'e', 's', 'b', 'e', 'b', 'e', 'b', 'e', 's', 'b', 'e', 's', 'b', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "def run_model(all_seq, seq_length):\n",
    "    \"\"\"\n",
    "    加载训练好的模型，获取模型的输入和输出节点。\n",
    "    :param all_seq:格式：输入序列\n",
    "    :param seq_length:输入序列长度\n",
    "    :return pred1,pred2:输出序列tf.argmax只有的标签；输出序列softmax之后的标签\n",
    "    \"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # 加载存储路径\n",
    "        check_point_path = 'checkpoint/'\n",
    "        # 找到模型文件名\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=check_point_path)\n",
    "\n",
    "        #ckpt有model_checkpoint_path和all_model_checkpoint_paths两个属性，分别是最新的模型和所有的模型\n",
    "        last_but_one_ckpt = ckpt.all_model_checkpoint_paths[-2]#选择倒数第二个模型\n",
    "        \n",
    "        #import_meta_graph将保存在.meta文件中的图添加到当前的图中\n",
    "        saver = tf.train.import_meta_graph(last_but_one_ckpt+'.meta')\n",
    "        \n",
    "        #从模型中恢复参数\n",
    "        saver.restore(sess, last_but_one_ckpt)\n",
    "        \n",
    "        input_placeholder = tf.get_default_graph().get_tensor_by_name(\"word_ids:0\")  # [batch_size, 200, 200, 3]\n",
    "\n",
    "        keep_prob_placeholder = tf.get_default_graph().get_tensor_by_name(\"dropout:0\")  # [batch_size, 200, 200, 3]\n",
    "        \n",
    "        sequence_lengths = tf.get_default_graph().get_tensor_by_name(\"sequence_lengths:0\")  # [batch_size, 200, 200, 3]\n",
    "        \n",
    "        output_1 = tf.get_default_graph().get_tensor_by_name(\"evaluation/Cast_1:0\")\n",
    "        \n",
    "        output_2 = tf.get_default_graph().get_tensor_by_name(\"evaluation/Softmax:0\")\n",
    "\n",
    "        pred1,pred2 = sess.run([output_1,output_2], \n",
    "                               feed_dict={input_placeholder: all_seq, sequence_lengths:seq_length, keep_prob_placeholder:1.0})\n",
    "        \n",
    "        return pred1,pred2\n",
    "\n",
    "def make_inputs(words):\n",
    "    \"\"\"\n",
    "    将数据pad固定长度。\n",
    "    :param words:预测样本\n",
    "    :return sent_new:padding后的输入序列\n",
    "    \"\"\"\n",
    "    pad = word2id['<PAD>']\n",
    "    all_seq = [word2id[word] for word in words]\n",
    "    sent_new = np.concatenate((all_seq, np.tile(pad, 32 - len(all_seq))), axis=0)\n",
    "    sent_new = np.reshape(sent_new, (1, 32))\n",
    "    return sent_new\n",
    "    \n",
    "\n",
    "def prediction_res(words,pred):\n",
    "    \"\"\"\n",
    "    打印分词语句序列，及其对应的标注序列\n",
    "    :param words:预测样本\n",
    "    :param pred:预测标注序列\n",
    "    :return tag_res,new_sentence:标记序列，分词后的句子\n",
    "    \"\"\"\n",
    "    id2tag = {tag2id[key]:key for key in tag2id}\n",
    "    tag_res = [id2tag[pred[0][i]] for i in range(len(words))]\n",
    "    \n",
    "    new_sentence = \"\"\n",
    "    index = 0\n",
    "    for i in range(len(tag_res)):\n",
    "        if tag_res[i] == \"s\":\n",
    "            new_sentence += words[i]\n",
    "            new_sentence += \"  \"\n",
    "        elif tag_res[i] == \"b\":\n",
    "            index = i\n",
    "        elif tag_res[i] == \"e\":\n",
    "            new_sentence += words[index:i+1]\n",
    "            new_sentence += \"  \"\n",
    "        else:\n",
    "            pass\n",
    "    return (tag_res,new_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# words没有做新词，默认所有词都有，不能写空格\n",
    "# words = '我家住在北京天安门'\n",
    "# words = '我爱你中国'\n",
    "# words = '王军虎去广州了'\n",
    "# words = '在北京大学生活区喝进口红酒'\n",
    "# words = '学生会宣传部'\n",
    "# words = '沿海南方向逃跑'\n",
    "# words = '这样的人才能经受住考验'\n",
    "# words = '网曝徐峥夜会美女'\n",
    "words = \"谢娜与子怡正面较量，国外谁的名气更大，看到老外的反应就知道了\"\n",
    "sent_new = make_inputs(words)\n",
    "pred1,pred2 = run_model(sent_new,[32])\n",
    "tag_res,new_sentence = prediction_res(words, pred1)\n",
    "\n",
    "print(words)\n",
    "print(new_sentence)\n",
    "print(tag_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以发现具有不符合逻辑的预测，'s'后面跟着'm'，'e'后面跟着'e'等，导致分词出现上述异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viterbi规范化输出标记序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 转移概率，单纯用等概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#状态转移矩阵\n",
    "trans_p = {'b': {'b': 0.0, 'm': 0.5, 'e': 0.5, 's': 0.0,'x':0.0},\n",
    "           'm': {'b': 0.0, 'm': 0.5, 'e': 0.5, 's': 0.0,'x':0.0},\n",
    "           'e': {'b': 0.5, 'm': 0.0, 'e': 0.0, 's': 0.5,'x':0.5},\n",
    "           's': {'b': 0.5, 'm': 0.0, 'e': 0.0, 's': 0.5,'x':0.5},\n",
    "           'x': {'b': 0.0, 'm': 0.0, 'e': 0.0, 's': 0.0,'x':0.5}}\n",
    "#初始状态\n",
    "start_p = {'b': 1.0, 'm': 0.0, 'e': 0.0, 's': 1.0,'x':0.0}#初始状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "谢娜与子怡正面较量，国外谁的名气更大，看到老外的反应就知道了\n",
      "谢娜  与子怡  正面  较量  ，国外  谁  的  名气  更  大，  看到  老外  的  反应  就  知道  了  \n",
      "['b', 'e', 'b', 'm', 'e', 'b', 'e', 'b', 'e', 'b', 'm', 'e', 's', 's', 'b', 'e', 's', 'b', 'e', 'b', 'e', 'b', 'e', 's', 'b', 'e', 's', 'b', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "def viterbi(pred,states, start_p, trans_p):\n",
    "    \"\"\"\n",
    "    改造的viterbi解码，寻找最优且合法路径\n",
    "    :param pred:神经网络模型预测的状态序列\n",
    "    :param states:状态集合\n",
    "    :param start_p:初始状态\n",
    "    :param trans_p:状态转移矩阵\n",
    "    :return prob, path[state]:最优概率，最优合法路径\n",
    "    \"\"\"\n",
    "    V = [{}]#存储状态对应的概率值\n",
    "    path = {}#保存总路径\n",
    "    \n",
    "    #初始状态\n",
    "    for y in range(len(states)):#['b','e','s','m','x']\n",
    "        V[0][states[y]] = start_p[states[y]] * pred[0][y]\n",
    "        path[states[y]] = [states[y]]\n",
    "    #寻找最优路径    \n",
    "    for t in range(1,len(pred)):\n",
    "        V.append({})\n",
    "        newpath = {}#保存当前层的全部最优路径\n",
    "        \n",
    "        for i in range(len(states)):#遍历当前层[0,1,2,3,4]\n",
    "            prob = 0.0\n",
    "            state = []\n",
    "            for s in states:#遍历前一层['b','e','s','m','x']\n",
    "                if V[t-1][s]>0:\n",
    "                    if prob < V[t-1][s] * trans_p[s][states[i]] * pred[t][i]:#选择当前节点与前面节点最优的概率及路径\n",
    "                        prob , state = V[t-1][s] * trans_p[s][states[i]] * pred[t][i] , s#保存当前节点与前面节点最优的概率及路径\n",
    "            V[t][states[i]] = prob\n",
    "            newpath[states[i]] = path[state] + [states[i]]\n",
    "        path = newpath\n",
    "\n",
    "    (prob, state) = max([(V[len(pred) - 1][y], y) for y in states])\n",
    "    return (prob, path[state])\n",
    "     \n",
    "\n",
    "def cut1(sentence,pred2):\n",
    "    \"\"\"\n",
    "    改造的viterbi解码，寻找最优且合法路径\n",
    "    :param pred:神经网络模型预测的状态序列\n",
    "    :param states:状态集合\n",
    "    :param start_p:初始状态\n",
    "    :param trans_p:状态转移矩阵\n",
    "    :return prob, path[state]:最优概率，最优合法路径\n",
    "    \"\"\"\n",
    "    prob, pos_list =  viterbi(pred2[0],['b','e','s','m','x'],start_p, trans_p)\n",
    "    pos_list = pos_list[0:len(sentence)]#预测序列中截取真是输入序列的长度\n",
    "    new_sentence = \"\"\n",
    "    index = 0\n",
    "    for i in range(len(pos_list)):\n",
    "        if pos_list[i] == \"s\":\n",
    "            new_sentence += sentence[i]\n",
    "            new_sentence += \"  \"\n",
    "        elif pos_list[i] == \"b\":\n",
    "            index = i\n",
    "        elif pos_list[i] == \"e\":\n",
    "            new_sentence += sentence[index:i+1]\n",
    "            new_sentence += \"  \"\n",
    "        else:\n",
    "            pass\n",
    " \n",
    "    return (prob,pos_list,new_sentence)\n",
    "\n",
    "\n",
    "words = \"谢娜与子怡正面较量，国外谁的名气更大，看到老外的反应就知道了\"\n",
    "prob,pos_list,new_sentence = cut1(words,pred2)\n",
    "\n",
    "print(words)\n",
    "print(new_sentence)\n",
    "print(pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以发现已经没有不符合逻辑的预测了，例如：'s'后面跟着'm'，'e'后面跟着'e'等。\n",
    "* 虽然分词效果还不理想，由于训练轮次紧训练了1轮，准确率在86%。在GPU上测试训练两轮就可以到90%，同样分词效果也会有所提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['b', 'e', 's', 'm', 'e', 'b', 'e', 'b', 'e', 'b', 'e', 'e', 's', 's', 'b', 'e', 's', 'b', 'e', 'b', 'e', 'b', 'e', 's', 'b', 'e', 's', 'b', 'e', 's']\n",
    "\n",
    "\n",
    "['b', 'e', 'b', 'm', 'e', 'b', 'e', 'b', 'e', 'b', 'm', 'e', 's', 's', 'b', 'e', 's', 'b', 'e', 'b', 'e', 'b', 'e', 's', 'b', 'e', 's', 'b', 'e', 's']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
